{"cells":[{"cell_type":"code","execution_count":1,"id":"uZR3iGJJtdDE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15695,"status":"ok","timestamp":1687320115036,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"uZR3iGJJtdDE","outputId":"fb3a9bf9-3819-46a9-b7e7-256fdf440f3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install -qU langchain openai"]},{"cell_type":"markdown","id":"7a4ba72d","metadata":{"id":"7a4ba72d"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb)\n","\n","#### [LangChain Handbook](https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook)\n","\n","# Getting Started with Chains\n","\n","Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n","\n","The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain — a `PromptTemplate` — to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain — a LLM.\n","\n","We'll start by importing all the libraries that we'll be using in this example."]},{"cell_type":"code","execution_count":2,"id":"66fb9c2a","metadata":{"executionInfo":{"elapsed":5179,"status":"ok","timestamp":1687320120211,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"66fb9c2a"},"outputs":[],"source":["import inspect\n","import re\n","\n","from getpass import getpass\n","from langchain import OpenAI, PromptTemplate\n","from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n","from langchain.callbacks import get_openai_callback"]},{"cell_type":"code","execution_count":2,"id":"pp7auliFGSgD","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687320120211,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"pp7auliFGSgD"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"wPdWz1IdxyBR","metadata":{"id":"wPdWz1IdxyBR"},"source":["To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted."]},{"cell_type":"code","execution_count":8,"id":"v86cmyppxdfc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1975,"status":"ok","timestamp":1687320211012,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"v86cmyppxdfc","outputId":"ecabeb81-3ee8-40e3-87d4-f6159b702b55"},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["OPENAI_API_KEY = getpass()"]},{"cell_type":"code","execution_count":9,"id":"baaa74b8","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687320213567,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"baaa74b8"},"outputs":[],"source":["llm = OpenAI(\n","    temperature=0,\n","    openai_api_key=OPENAI_API_KEY\n","    )"]},{"cell_type":"markdown","id":"309g_2pqxzzB","metadata":{"id":"309g_2pqxzzB"},"source":["An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."]},{"cell_type":"code","execution_count":11,"id":"DsC3szr6yP3L","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1687320225401,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"DsC3szr6yP3L"},"outputs":[],"source":["def count_tokens(chain, query):\n","    with get_openai_callback() as cb:\n","        result = chain.run(query)\n","        print(f'Spent a total of {cb.total_tokens} tokens')\n","\n","    return result"]},{"cell_type":"markdown","id":"6e1f31b4","metadata":{"id":"6e1f31b4"},"source":["## What are chains anyway?"]},{"cell_type":"markdown","id":"5b919c3a","metadata":{"id":"5b919c3a"},"source":["**Definition**: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n","\n","The official definition of chains is the following:\n","\n","\n","> A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains.\n","\n","\n","So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."]},{"cell_type":"markdown","id":"c4644b2f","metadata":{"id":"c4644b2f"},"source":["Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n","\n","1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n","2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."]},{"cell_type":"markdown","id":"e4d283b6","metadata":{"id":"e4d283b6"},"source":["Let's take a peek into what these chains have to offer!"]},{"cell_type":"markdown","id":"831827b7","metadata":{"id":"831827b7"},"source":["### Utility Chains"]},{"cell_type":"markdown","id":"6c66e4b4","metadata":{"id":"6c66e4b4"},"source":["Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"]},{"cell_type":"markdown","id":"HF3XCWD2sVi0","metadata":{"id":"HF3XCWD2sVi0"},"source":["#### Pro-tip: use `verbose=True` to see what the different steps in the chain are!"]},{"cell_type":"code","execution_count":12,"id":"b4161561","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"elapsed":6680,"status":"ok","timestamp":1687320232984,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"b4161561","outputId":"26f4ab47-815c-4102-da47-ec9565fde46e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new  chain...\u001b[0m\n","What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m\n","```text\n","13**.3432\n","```\n","...numexpr.evaluate(\"13**.3432\")...\n","\u001b[0m\n","Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n","\u001b[1m> Finished chain.\u001b[0m\n","Spent a total of 264 tokens\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Answer: 2.4116004626599237'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["llm_math = LLMMathChain(llm=llm, verbose=True)\n","\n","count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"]},{"cell_type":"markdown","id":"198eebb2","metadata":{"id":"198eebb2"},"source":["Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code?"]},{"cell_type":"markdown","id":"a7a0821a","metadata":{"id":"a7a0821a"},"source":["**Enter prompts**"]},{"cell_type":"markdown","id":"c86c5798","metadata":{"id":"c86c5798"},"source":["The question we send as input to the chain is not the only input that the llm recieves 😉. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"]},{"cell_type":"code","execution_count":13,"id":"62778ef4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1687320241331,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"62778ef4","outputId":"cf7d6f88-95cd-4130-efac-4109946b585f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n","\n","Question: ${{Question with math problem.}}\n","```text\n","${{single line mathematical expression that solves the problem}}\n","```\n","...numexpr.evaluate(text)...\n","```output\n","${{Output of running the code}}\n","```\n","Answer: ${{Answer}}\n","\n","Begin.\n","\n","Question: What is 37593 * 67?\n","```text\n","37593 * 67\n","```\n","...numexpr.evaluate(\"37593 * 67\")...\n","```output\n","2518731\n","```\n","Answer: 2518731\n","\n","Question: 37593^(1/5)\n","```text\n","37593**(1/5)\n","```\n","...numexpr.evaluate(\"37593**(1/5)\")...\n","```output\n","8.222831614237718\n","```\n","Answer: 8.222831614237718\n","\n","Question: {question}\n","\n"]}],"source":["print(llm_math.prompt.template)"]},{"cell_type":"markdown","id":"708031d8","metadata":{"id":"708031d8"},"source":["Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! 🧐"]},{"cell_type":"code","execution_count":14,"id":"66b92768","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1687320246973,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"66b92768","outputId":"85c0a26e-c92a-47b7-aba4-127913645031"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spent a total of 17 tokens\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n\\n2.907'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# we set the prompt to only have the question we ask\n","prompt = PromptTemplate(input_variables=['question'], template='{question}')\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","\n","# we ask the llm for the answer with no context\n","\n","count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"]},{"cell_type":"markdown","id":"d147e7bf","metadata":{"id":"d147e7bf"},"source":["Wrong answer! Herein lies the power of prompting and one of our most important insights so far:\n","\n","**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"]},{"cell_type":"markdown","id":"1cd2a31f","metadata":{"id":"1cd2a31f"},"source":["Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."]},{"cell_type":"code","execution_count":null,"id":"3488c5b6","metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1687320168251,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"3488c5b6"},"outputs":[],"source":["print(inspect.getsource(llm_math._call))"]},{"cell_type":"markdown","id":"fa6b6c2e","metadata":{"id":"fa6b6c2e"},"source":["So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"]},{"cell_type":"markdown","id":"67f96bd3","metadata":{"id":"67f96bd3"},"source":["Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."]},{"cell_type":"markdown","id":"b109619a","metadata":{"id":"b109619a"},"source":["Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly 🔥\n","\n","The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html) and tinker with the example notebooks that you might find interesting."]},{"cell_type":"markdown","id":"381e329c","metadata":{"id":"381e329c"},"source":["*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"]},{"cell_type":"markdown","id":"f66a25a2","metadata":{"id":"f66a25a2"},"source":["### Generic chains"]},{"cell_type":"markdown","id":"70b32a84","metadata":{"id":"70b32a84"},"source":["There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"]},{"cell_type":"markdown","id":"4b8e2048","metadata":{"id":"4b8e2048"},"source":["Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat 😉"]},{"cell_type":"markdown","id":"a6e778d2","metadata":{"id":"a6e778d2"},"source":["First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."]},{"cell_type":"code","execution_count":15,"id":"c794e00a","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1687320269165,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"c794e00a"},"outputs":[],"source":["def transform_func(inputs: dict) -> dict:\n","    text = inputs[\"text\"]\n","\n","    # replace multiple new lines and multiple spaces with a single one\n","    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n","    text = re.sub(r'[ \\t]+', ' ', text)\n","\n","    return {\"output_text\": text}"]},{"cell_type":"markdown","id":"42dc1ac6","metadata":{"id":"42dc1ac6"},"source":["Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."]},{"cell_type":"code","execution_count":16,"id":"286f7295","metadata":{"executionInfo":{"elapsed":150,"status":"ok","timestamp":1687320275892,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"286f7295"},"outputs":[],"source":["clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"]},{"cell_type":"code","execution_count":17,"id":"977bf11a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":65,"status":"ok","timestamp":1687320287080,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"977bf11a","outputId":"6ff9d308-dba7-4ebf-b3ea-11df29ee6731"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'A random text with some irregular spacing.\\n Another one here as well.'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"]},{"cell_type":"markdown","id":"b3f84cd0","metadata":{"id":"b3f84cd0"},"source":["Great! Now things will get interesting.\n","\n","Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"]},{"cell_type":"markdown","id":"5b77042a","metadata":{"id":"5b77042a"},"source":["First we will build the prompt template:"]},{"cell_type":"code","execution_count":18,"id":"73719a5d","metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1687320287082,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"73719a5d"},"outputs":[],"source":["template = \"\"\"Paraphrase this text:\n","\n","{output_text}\n","\n","In the style of a {style}.\n","\n","Paraphrase: \"\"\"\n","prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"]},{"cell_type":"markdown","id":"83b2ec83","metadata":{"id":"83b2ec83"},"source":["And next, initialize our chain:"]},{"cell_type":"code","execution_count":19,"id":"48a067ab","metadata":{"executionInfo":{"elapsed":66,"status":"ok","timestamp":1687320292640,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"48a067ab"},"outputs":[],"source":["style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"]},{"cell_type":"markdown","id":"2324005d","metadata":{"id":"2324005d"},"source":["Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n","\n","We are going to pass the output of the `TransformChain` to the `LLMChain`!"]},{"cell_type":"markdown","id":"c5da4925","metadata":{"id":"c5da4925"},"source":["Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."]},{"cell_type":"code","execution_count":20,"id":"06f51f17","metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1687320292644,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"06f51f17"},"outputs":[],"source":["sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"]},{"cell_type":"markdown","id":"7f0f51d8","metadata":{"id":"7f0f51d8"},"source":["Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."]},{"cell_type":"code","execution_count":21,"id":"a8032489","metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1687320292657,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"a8032489"},"outputs":[],"source":["input_text = \"\"\"\n","Chains allow us to combine multiple\n","\n","\n","components together to create a single, coherent application.\n","\n","For example, we can create a chain that takes user input,       format it with a PromptTemplate,\n","\n","and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by\n","\n","\n","combining chains with other components.\n","\"\"\""]},{"cell_type":"markdown","id":"b2f55d21","metadata":{"id":"b2f55d21"},"source":["We are all set. Time to get creative!"]},{"cell_type":"code","execution_count":22,"id":"d507aa5c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"elapsed":4794,"status":"ok","timestamp":1687320309392,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"d507aa5c","outputId":"7cecb4c3-8869-4e4b-c16d-a87e37795fba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spent a total of 158 tokens\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nChains let us link up multiple pieces to make one dope app. Like, we can take user input, style it with a PromptTemplate, then pass it to an LLM. We can get even more creative by combining multiple chains or mixin' chains with other components.\""]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"]},{"cell_type":"markdown","id":"60b52e19","metadata":{"id":"60b52e19"},"source":["## A note on langchain-hub"]},{"cell_type":"markdown","id":"02f649da","metadata":{"id":"02f649da"},"source":["`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use."]},{"cell_type":"code","execution_count":23,"id":"411500c2","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687320317925,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"411500c2"},"outputs":[],"source":["from langchain.chains import load_chain"]},{"cell_type":"markdown","id":"b375e5b7","metadata":{"id":"b375e5b7"},"source":["Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:"]},{"cell_type":"code","execution_count":25,"id":"fbe8748d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1487,"status":"ok","timestamp":1687320429789,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"fbe8748d","outputId":"ca3bf3fa-7793-43b0-c6ff-39f2b01f9319"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n","  warnings.warn(\n"]}],"source":["#@title Setting up OpenAI API\n","import os\n","os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n","llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"]},{"cell_type":"markdown","id":"ebcfe67c","metadata":{"id":"ebcfe67c"},"source":["What if we want to change some of the configuration parameters? We can simply override it after loading:"]},{"cell_type":"code","execution_count":26,"id":"d0d54233","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1687320439757,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"d0d54233","outputId":"7362f3eb-6355-4f45-807b-044244f3e2ca"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["llm_math_chain.verbose"]},{"cell_type":"code","execution_count":27,"id":"074f8806","metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1687320440493,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"074f8806"},"outputs":[],"source":["llm_math_chain.verbose = False"]},{"cell_type":"code","execution_count":28,"id":"465a6cbf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1687320440495,"user":{"displayName":"Clay Boss","userId":"09804780439396989080"},"user_tz":420},"id":"465a6cbf","outputId":"efc30ae4-032e-4a9c-b352-278aed9a513e"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["llm_math_chain.verbose"]},{"cell_type":"markdown","id":"2cc688ca","metadata":{"id":"2cc688ca"},"source":["That's it for this example on chains.\n","\n","---"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb","timestamp":1687320031491}]},"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"}}},"nbformat":4,"nbformat_minor":5}
